{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6BytytWcwa4hLwJZ2SKFx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slilli23/Horcynus-orca-stile/blob/main/Horcynus_Orca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Esplorare l'anomalia: uno studio computazionale dello stile di Horcinus Orca\n",
        "di Silvia Lilli"
      ],
      "metadata": {
        "id": "RqGvf6P5DJAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il contributo propone un approccio innovativo allo studio linguistico di Horcynus Orca, integrando lettura ravvicinata e strumenti della linguistica computazionale. L’esplorazione del testo tramite la codifica di espressioni regolari (REGEX) consente di superare l’ostacolo costituito dall’estensione e complessità del romanzo – che ha spesso imposto un approccio esemplificativo – e incoraggia una formalizzazione in linea con la «grammaticalizzazione» dell’elemento deformante riconosciuta all’autore. L’analisi si concentra, a titolo esemplificativo, su due strategie inventive: le formazioni verbali parasintetiche riflessive e i reduplicati con funzione elativa. Il confronto con repertori esistenti ha rivelato nuove occorrenze riconducibili alla creatività autoriale, confermando che l’approccio integrato tra *distant* e *close reading* garantisce un’analisi più precisa e completa, aprendo nuove possibilità interpretative."
      ],
      "metadata": {
        "id": "7gks4xvCjoLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definizione delle REGEX per identificare i fenomeni di interesse nel testo:\n",
        "\n",
        "*   Formazioni verbali parasintetiche riflessive con prefisso in A(D)-\n",
        "\n",
        "    **\\ba([bcdfglmnpqrstvz])\\1[a-zà-ù]{4,}[ro]si\\b**\n",
        "\n",
        "    **\\b(s[’']a([bcdfglmnpqrstvz])\\2[a-zà-ù]{4,})**\n",
        "\n",
        "\n",
        "*   Reduplicati con funzione elativa\n",
        "\n",
        "    **\\b(\\w+)[\\s]?\\1\\b**\n",
        "\n",
        "Si procede quindi all'importazione del testo in formato .txt e all'estrazione dei risultati attraverso l'applicazione delle REGEX.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "get9_eYgES0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "imKyUepwjecw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTjbJf6pDFWM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2917dc2f-a1a3-428e-bdac-869a8280fbaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'Horcynus-orca-ADI2025'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 20 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (20/20), 1.13 MiB | 5.92 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n",
            "/content/Horcynus-orca-ADI2025\n",
            "Root: /content/Horcynus-orca-ADI2025\n",
            "Data: /content/Horcynus-orca-ADI2025/data\n",
            "Scripts: /content/Horcynus-orca-ADI2025/scripts\n",
            "Results: /content/Horcynus-orca-ADI2025/results\n",
            "Lunghezza testo: 3078146 caratteri\n",
            "Primi 500 caratteri:\n",
            " Il sole tramontò quattro volte sul suo viaggio e alla fine del quarto giorno, che era il quattro di ottobre del millenovecentoquarantatre, il marinaio, nocchiero semplice della fu regia Marina ’Ndrja Cambrìa arrivò al paese delle Femmine, sui mari dello scill’e cariddi.\n",
            "\n",
            "Imbruniva a vista d’occhio e un filo di ventilazione alitava dal mare in rema sul basso promontorio. Per tutto quel giorno il mare si era allisciato ancora alla grande calmerìa di scirocco che durava, senza mutamento alcuno, sin\n"
          ]
        }
      ],
      "source": [
        "# Clona il la cartella dal repository GitHub\n",
        "REPO_URL  = \"https://github.com/slilli23/Horcynus-orca-ADI2025.git\"\n",
        "REPO_NAME = \"Horcynus-orca-ADI2025\"\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Torna alla radice di Colab per sicurezza\n",
        "%cd /content\n",
        "\n",
        "if (Path(REPO_NAME) / \".git\").exists():\n",
        "    %cd {REPO_NAME}\n",
        "    !git pull --ff-only\n",
        "else:\n",
        "    !git clone --depth=1 {REPO_URL}\n",
        "    %cd {REPO_NAME}\n",
        "\n",
        "# Importa le librerie necessarie per l'analisi\n",
        "from pathlib import Path\n",
        "import re\n",
        "import pandas as pd\n",
        "import csv\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# Definisci le cartelle\n",
        "ROOT = Path.cwd()\n",
        "DATA_DIR = ROOT / \"data\"\n",
        "SCRIPTS_DIR = ROOT / \"scripts\"\n",
        "RESULTS_DIR = ROOT / \"results\"\n",
        "\n",
        "#Verifica l'esistenza delle cartelle e la correttezza dei percorsi\n",
        "RESULTS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"Root:\", ROOT)\n",
        "print(\"Data:\", DATA_DIR)\n",
        "print(\"Scripts:\", SCRIPTS_DIR)\n",
        "print(\"Results:\", RESULTS_DIR)\n",
        "\n",
        "# Carica il testo\n",
        "file_path = DATA_DIR / \"Horcynus_orca.txt\"\n",
        "\n",
        "with open(file_path, encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Lunghezza testo:\", len(text), \"caratteri\")\n",
        "print(\"Primi 500 caratteri:\\n\", text[:500])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parasintetici con prefisso in A(D)-"
      ],
      "metadata": {
        "id": "wHzPAz-fjVuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applichiamo la prima REGEX (verbi parasintetici riflessivi con prefisso in ad-, forma indefinita)."
      ],
      "metadata": {
        "id": "-1hTqvjRKv0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imposta la cartella di output\n",
        "OUTPUT_DIR = Path.cwd() / \"results\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Salva la REGEX in una variabile (raw string per non dover raddoppiare i backslash)\n",
        "pattern = r\"\\ba([bcdfglmnpqrstvz])\\1[a-zà-ù]{4,}[ro]si\\b\"\n",
        "rx = re.compile(pattern, flags=re.IGNORECASE | re.UNICODE)\n",
        "\n",
        "# Applica la REGEX e salva il match con il con contesto (5 parole prima e dopo)\n",
        "words = text.split()\n",
        "matches = []\n",
        "\n",
        "TRAIL_PUNCT_RE = re.compile(r\"[.,;:!?…]+$\")  # punteggiatura finale\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "    # 1) rimuovi punteggiatura finale\n",
        "    token = TRAIL_PUNCT_RE.sub(\"\", word)\n",
        "\n",
        "    # 2) separa per apostrofo\n",
        "    parts = re.split(r\"[’']\", token)\n",
        "\n",
        "    found_for_token = False\n",
        "    for part in parts:\n",
        "        if not part:\n",
        "            continue\n",
        "        if rx.search(part):\n",
        "            left_context = \" \".join(words[max(0, i-5):i])\n",
        "            right_context = \" \".join(words[i+1:i+6])\n",
        "            # salva SOLO la parte dopo l'apostrofo che ha matchato\n",
        "            matches.append([part, left_context, right_context])\n",
        "            found_for_token = True\n",
        "            break  # evita duplicati se la stessa parola produce più sottoparti\n",
        "\n",
        "# Definisci il file di output\n",
        "OUTPUT_PATH = OUTPUT_DIR / \"parasintetici_indefiniti.csv\"\n",
        "\n",
        "# Salva i risultati in CSV\n",
        "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(OUTPUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"match\", \"left_context\", \"right_context\"])\n",
        "    writer.writerows(matches)\n",
        "\n",
        "print(f\"Salvati {len(matches)} risultati in {OUTPUT_PATH}\")\n"
      ],
      "metadata": {
        "id": "zEKq5_Rw5U6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0148089f-9217-42e9-b5b0-6efa9ae83d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Salvati 267 risultati in /content/Horcynus-orca-ADI2025/results/parasintetici_indefiniti.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifichiamo se le parole trovate appartengono al vocabolario italiano, utilizzando la libreria spaCy. Per prima cosa scarichiamo il modello utilizzato da spaCy. Viene scelto il modello 'large', per ottenere una prestazione più accurata."
      ],
      "metadata": {
        "id": "DO8DU0DJgBVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installazione\n",
        "!pip -q install -U pip setuptools wheel\n",
        "!pip -q install \"numpy==1.26.4\" \"spacy==3.7.5\"\n",
        "!python -m spacy download it_core_news_lg\n",
        "\n",
        "\n",
        "# --- Setup ---\n",
        "import spacy"
      ],
      "metadata": {
        "id": "cmYacxvSq9vI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a70eff3d-2008-4050-b2cd-d099ed6d1846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting it-core-news-lg==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_lg-3.7.0/it_core_news_lg-3.7.0-py3-none-any.whl (567.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m567.9/567.9 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m  \u001b[33m0:00:09\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from it-core-news-lg==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (0.17.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (2.11.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (80.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (2025.8.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.12/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (1.17.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->it-core-news-lg==3.7.0) (3.0.2)\n",
            "Installing collected packages: it-core-news-lg\n",
            "Successfully installed it-core-news-lg-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4056399038.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# --- Setup ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# set library-specific custom warning handling before doing anything else\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup_default_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msetup_default_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/errors.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mcatalogue\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_importlib_metadata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimportlib_metadata\u001b[0m  \u001b[0;31m# type: ignore[no-redef]    # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptimizer\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mpickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/thinc/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from .backends import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mCupyOps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mMPSOps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mNumpyOps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mOps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/thinc/backends/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_cupy_allocators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcupy_pytorch_allocator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcupy_tensorflow_allocator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_param_server\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParamServer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcupy_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCupyOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmps_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMPSOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnumpy_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpyOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/thinc/backends/cupy_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_custom_kernels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnumpy_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpyOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/thinc/backends/numpy_ops.pyx\u001b[0m in \u001b[0;36minit thinc.backends.numpy_ops\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Riavviamo il runtime per risolvere i problemi di dipendenze (Ctrl+M)."
      ],
      "metadata": {
        "id": "Jy1J73kiZcYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup ---\n",
        "import spacy"
      ],
      "metadata": {
        "id": "Yna2KoTYTBL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Annotiamo il testo con la libreria spaCy."
      ],
      "metadata": {
        "id": "wjZvcLKXZo1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = Path.cwd().resolve()\n",
        "IN_PATH  = ROOT / \"results\" / \"parasintetici_indefiniti.csv\"\n",
        "OUT_PATH = ROOT / \"results\" / \"parasintetici_indefiniti_spacy.csv\"\n",
        "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "nlp = spacy.load(\"it_core_news_lg\")\n",
        "\n",
        "TRAIL_PUNCT_RE = re.compile(r\"[.,;:!?…]+$\")\n",
        "\n",
        "def variants(w: str):\n",
        "    w0 = w.strip()\n",
        "    w0 = TRAIL_PUNCT_RE.sub(\"\", w0)\n",
        "    outs = set([w0, w0.lower(), w0.replace(\"’\",\"'\")])\n",
        "    # se riflessivo in -rsi prova senza 'si' (infinito)\n",
        "    wl = w0.lower().replace(\"’\",\"'\")\n",
        "    if wl.endswith(\"rsi\"):\n",
        "        outs.add(wl[:-2])\n",
        "    # rimuovi apostrofo iniziale di clitici frequenti\n",
        "    for pref in (\"l'\", \"s'\", \"m'\", \"t'\", \"d'\", \"n'\"):\n",
        "        if wl.startswith(pref):\n",
        "            outs.add(wl[len(pref):])\n",
        "    return outs\n",
        "\n",
        "def spacy_check(word: str):\n",
        "    # ordina preferendo la forma originale e la minuscola\n",
        "    cand = sorted(list(variants(word)), key=lambda x: (x != word, x != word.lower()))\n",
        "    known = False\n",
        "    chosen = None\n",
        "    for v in cand:\n",
        "        if not nlp.vocab[v].is_oov:\n",
        "            known = True\n",
        "            chosen = v\n",
        "            break\n",
        "    form = chosen or cand[0]\n",
        "    doc = nlp(form)\n",
        "     # usa il primo token \"parola\" (salta eventuali segni)\n",
        "    tok = next((t for t in doc if t.is_alpha or t.text.strip(\"'\")), doc[0])\n",
        "    return known, form, tok.lemma_\n",
        "\n",
        "rows_out = []\n",
        "with open(IN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    r = csv.DictReader(f)\n",
        "    for row in r:\n",
        "        w = row[\"match\"]\n",
        "        known, chosen, lemma = spacy_check(w)\n",
        "        row[\"spacy_known\"]   = known\n",
        "        row[\"chosen_variant\"] = chosen    # forma su cui è stato fatto il check\n",
        "        row[\"lemma\"]          = lemma\n",
        "        rows_out.append(row)\n",
        "\n",
        "# Scrivi output\n",
        "with open(OUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    fieldnames = list(rows_out[0].keys())\n",
        "    w = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "    w.writeheader()\n",
        "    w.writerows(rows_out)\n",
        "\n",
        "print(f\"Annotate {len(rows_out)} forme. File scritto in: {OUT_PATH}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "KU_Pfpaqc7sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accorpiamo le voci in types distinguendo le voci omografe che hanno avuto diverse etichette generate da spaCy."
      ],
      "metadata": {
        "id": "2BCojsCfbAjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conta per (type, spacy_known)\n",
        "counter = defaultdict(int)\n",
        "\n",
        "for row in rows_out:\n",
        "    type_form = row[\"match\"].strip().lower()\n",
        "    spacy_status = row[\"spacy_known\"]\n",
        "    counter[(type_form, spacy_status)] += 1\n",
        "\n",
        "# Crea DataFrame\n",
        "aggregated = []\n",
        "for (type_form, spacy_status), freq in sorted(counter.items()):\n",
        "    aggregated.append({\n",
        "        \"type\": type_form,\n",
        "        \"spacy_known\": spacy_status,\n",
        "        \"freq\": freq\n",
        "    })\n",
        "\n",
        "df_out = pd.DataFrame(aggregated)\n",
        "df_out = df_out.sort_values(by=[\"freq\", \"type\"], ascending=[False, True]).reset_index(drop=True)\n",
        "\n",
        "# --- Salva\n",
        "AGG_OUT_PATH = ROOT / \"results\" / \"parasintetici_indefiniti_types_spacy.csv\"\n",
        "df_out.to_csv(AGG_OUT_PATH, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Salvati {len(df_out)} types distinti con spacy_known in {AGG_OUT_PATH}\")\n"
      ],
      "metadata": {
        "id": "mfCM9NqwbBG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applichiamo la seconda REGEX (forme parasintetiche riflessive con prefisso A(D)- nelle forme finite del verbo."
      ],
      "metadata": {
        "id": "VWKXvr5Ck9fT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imposta la cartella di output\n",
        "OUTPUT_DIR = Path.cwd() / \"results\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Salva la REGEX in una variabile (raw string per non dover raddoppiare i backslash)\n",
        "pattern = r\"\\b(s[’']a([bcdfglmnpqrstvz])\\2[a-zà-ù]{4,})\"\n",
        "rx = re.compile(pattern, flags=re.IGNORECASE | re.UNICODE)\n",
        "\n",
        "# Applica la REGEX e salva il match con il con contesto (5 parole prima e dopo)\n",
        "words = text.split()\n",
        "matches = []\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "    if rx.search(word):\n",
        "        left_context = \" \".join(words[max(0, i-5):i])\n",
        "        right_context = \" \".join(words[i+1:i+6])\n",
        "        matches.append([word, left_context, right_context])\n",
        "\n",
        "# Definisci il file di output\n",
        "OUTPUT_PATH = OUTPUT_DIR / \"parasintetici_finiti.csv\"\n",
        "\n",
        "# Salva i risultati in CSV\n",
        "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(OUTPUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"match\", \"left_context\", \"right_context\"])\n",
        "    writer.writerows(matches)\n",
        "\n",
        "print(f\"Salvati {len(matches)} risultati in {OUTPUT_PATH}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "DDqR6BCZc9Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Annotiamo il testo attraverso spaCy per verificare le forme riconosciute come italiane."
      ],
      "metadata": {
        "id": "MfzsaYLGcAXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = Path.cwd().resolve()\n",
        "IN_PATH  = ROOT / \"results\" / \"parasintetici_finiti.csv\"\n",
        "OUT_PATH = ROOT / \"results\" / \"parasintetici_finiti_spacy.csv\"\n",
        "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "nlp = spacy.load(\"it_core_news_lg\")\n",
        "\n",
        "_APOS = r\"[’']\"  # apostrofo semplice o tipografico\n",
        "S_PREFIX_RE = re.compile(rf\"^\\s*s{_APOS}\\s*(.+)\\s*$\", flags=re.IGNORECASE)\n",
        "TRAIL_PUNCT_RE = re.compile(r\"[.,;:!?…]+$\")\n",
        "\n",
        "# considera solo la parola dopo la sequenza s'\n",
        "def extract_verb_form(w: str) -> str:\n",
        "    w0 = w.strip()\n",
        "    w0 = TRAIL_PUNCT_RE.sub(\"\", w0)\n",
        "    m = S_PREFIX_RE.match(w0)\n",
        "    if m:\n",
        "        return m.group(1).strip()\n",
        "    return w0\n",
        "\n",
        "def variants(w: str):\n",
        "    w0 = extract_verb_form(w).strip()\n",
        "    return {w0, w0.lower()}\n",
        "\n",
        "\n",
        "def spacy_check(word: str):\n",
        "\n",
        "    cand = sorted(list(variants(word)), key=lambda x: (x != word, x != word.lower()))\n",
        "    known = False\n",
        "    chosen = None\n",
        "    for v in cand:\n",
        "        if not nlp.vocab[v].is_oov:\n",
        "            known = True\n",
        "            chosen = v\n",
        "            break\n",
        "    form = chosen or cand[0]\n",
        "    doc = nlp(form)\n",
        "    tok = next((t for t in doc if t.is_alpha or t.text.strip(\"'\")), doc[0])\n",
        "    return known, form, tok.lemma_\n",
        "\n",
        "rows_out = []\n",
        "with open(IN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    r = csv.DictReader(f)\n",
        "    for row in r:\n",
        "        w = row[\"match\"]\n",
        "        known, chosen, lemma = spacy_check(w)\n",
        "        row[\"spacy_known\"]   = known\n",
        "        row[\"chosen_variant\"] = chosen    # forma su cui è stato fatto il check\n",
        "        row[\"lemma\"]          = lemma\n",
        "        rows_out.append(row)\n",
        "\n",
        "# Scrivi output\n",
        "with open(OUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    fieldnames = list(rows_out[0].keys())\n",
        "    w = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "    w.writeheader()\n",
        "    w.writerows(rows_out)\n",
        "\n",
        "print(f\"Annotate {len(rows_out)} forme. File scritto in: {OUT_PATH}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "QGe-rJJVk7V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accorpiamo le voci in types distinguendo le voci omografe che hanno avuto diverse etichette generate da spaCy."
      ],
      "metadata": {
        "id": "J4VdbuDcaZ0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Normalizza e aggrega\n",
        "counter = defaultdict(int)  # chiave = (type, spacy_known), valore = frequenza\n",
        "\n",
        "for row in rows_out:\n",
        "    type_form = extract_verb_form(row[\"match\"]).lower().strip()\n",
        "    spacy_status = row[\"spacy_known\"]\n",
        "    counter[(type_form, spacy_status)] += 1\n",
        "\n",
        "# 2. Crea dataframe\n",
        "aggregated = []\n",
        "for (type_form, spacy_status), freq in sorted(counter.items()):\n",
        "    aggregated.append({\n",
        "        \"type\": type_form,\n",
        "        \"spacy_known\": spacy_status,\n",
        "        \"freq\": freq\n",
        "    })\n",
        "\n",
        "df_out = pd.DataFrame(aggregated)\n",
        "df_out = df_out.sort_values(by=[\"freq\", \"type\"], ascending=[False, True]).reset_index(drop=True)\n",
        "\n",
        "# 3. Salva CSV\n",
        "AGG_OUT_PATH = ROOT / \"results\" / \"parasintetici_finiti_types_spacy.csv\"\n",
        "df_out.to_csv(AGG_OUT_PATH, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Salvati {len(df_out)} types distinti con spacy_known in {AGG_OUT_PATH}\")"
      ],
      "metadata": {
        "id": "TVwckCsRaZGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applichiamo la REGEX per trovare nel testo anche le sequenze con \"si\" non eliso."
      ],
      "metadata": {
        "id": "-RKjJhNbapJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, csv\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Input: il testo intero in una stringa ---\n",
        "# text = ...\n",
        "\n",
        "# Imposta la cartella di output\n",
        "OUTPUT_DIR = Path.cwd() / \"results\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# REGEX per la seconda parola: a + C + C + almeno 4 lettere\n",
        "AWORD_RE = re.compile(r\"^a([bcdfglmnpqrstvz])\\1[a-zà-ù]{4,}$\", flags=re.IGNORECASE | re.UNICODE)\n",
        "\n",
        "# Pulisci punteggiatura iniziale/finale e normalizza apostrofi\n",
        "TRAIL_PUNCT_RE = re.compile(r\"[.,;:!?…)\\]»”]+$\")\n",
        "LEAD_PUNCT_RE  = re.compile(r\"^[([«“\\\"(]+\")\n",
        "\n",
        "def clean_token(w: str) -> str:\n",
        "    if w is None:\n",
        "        return \"\"\n",
        "    w = str(w)\n",
        "    w = LEAD_PUNCT_RE.sub(\"\", w)\n",
        "    w = TRAIL_PUNCT_RE.sub(\"\", w)\n",
        "    return w.replace(\"’\", \"'\").strip()\n",
        "\n",
        "# Tokenizza in modo semplice (per contesti bastano gli spazi)\n",
        "words = text.split()\n",
        "matches = []\n",
        "\n",
        "i = 0\n",
        "while i < len(words) - 1:\n",
        "    w0_raw = words[i]\n",
        "    w1_raw = words[i+1]\n",
        "\n",
        "    w0 = clean_token(w0_raw)\n",
        "    w1 = clean_token(w1_raw)\n",
        "\n",
        "    # cerchiamo \"si\" come parola intera (case-insensitive)\n",
        "    if w0.lower() == \"si\" and AWORD_RE.match(w1):\n",
        "        left_context  = \" \".join(words[max(0, i-5):i])\n",
        "        # >>> salva SOLO la seconda parola (pulita) <<<\n",
        "        match_text    = w1\n",
        "        right_context = \" \".join(words[i+2:i+7])\n",
        "        matches.append([match_text, left_context, right_context])\n",
        "        i += 2\n",
        "        continue\n",
        "\n",
        "    i += 1\n",
        "\n",
        "# Definisci il file di output\n",
        "OUTPUT_PATH = OUTPUT_DIR / \"parasintetici_finiti_si.csv\"\n",
        "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Salva i risultati in CSV\n",
        "with open(OUTPUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"match\", \"left_context\", \"right_context\"])\n",
        "    writer.writerows(matches)\n",
        "\n",
        "print(f\"Salvati {len(matches)} risultati in {OUTPUT_PATH}\")"
      ],
      "metadata": {
        "id": "1jkzi0n2aveN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Annotiamo i risultati con la libreria spaCy."
      ],
      "metadata": {
        "id": "wT__WqScd1-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = Path.cwd().resolve()\n",
        "IN_PATH  = ROOT / \"results\" / \"parasintetici_finiti_si.csv\"\n",
        "OUT_PATH = ROOT / \"results\" / \"parasintetici_finiti_si_spacy.csv\"\n",
        "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "nlp = spacy.load(\"it_core_news_lg\")\n",
        "\n",
        "def variants(w: str):\n",
        "    w0 = w.strip()\n",
        "    return {w0, w0.lower()}\n",
        "\n",
        "def spacy_check(word: str):\n",
        "    cand = sorted(list(variants(word)), key=lambda x: (x != word, x != word.lower()))\n",
        "    known = False\n",
        "    chosen = None\n",
        "    for v in cand:\n",
        "        if not nlp.vocab[v].is_oov:\n",
        "            known = True\n",
        "            chosen = v\n",
        "            break\n",
        "    form = chosen or cand[0]\n",
        "    doc = nlp(form)\n",
        "    tok = next((t for t in doc if t.is_alpha or t.text.strip(\"'\")), doc[0])\n",
        "    return known, form, tok.lemma_\n",
        "\n",
        "rows_out = []\n",
        "with open(IN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    r = csv.DictReader(f)\n",
        "    for row in r:\n",
        "        w = row[\"match\"]\n",
        "        known, chosen, lemma = spacy_check(w)\n",
        "        row[\"spacy_known\"]    = known\n",
        "        row[\"chosen_variant\"] = chosen\n",
        "        row[\"lemma\"]          = lemma\n",
        "        rows_out.append(row)\n",
        "\n",
        "with open(OUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    fieldnames = list(rows_out[0].keys())\n",
        "    w = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "    w.writeheader()\n",
        "    w.writerows(rows_out)\n",
        "\n",
        "print(f\"Annotate {len(rows_out)} forme. File scritto in: {OUT_PATH}\")\n"
      ],
      "metadata": {
        "id": "s2UKtNPSd1ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accorpiamo le voci in types distinguendo le voci omografe che hanno avuto diverse etichette generate da spaCy."
      ],
      "metadata": {
        "id": "4RpECIPBzdkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Normalizza e aggrega\n",
        "counter = defaultdict(int)  # chiave = (type, spacy_known), valore = frequenza\n",
        "\n",
        "for row in rows_out:\n",
        "    type_form = extract_verb_form(row[\"match\"]).lower().strip()\n",
        "    spacy_status = row[\"spacy_known\"]\n",
        "    counter[(type_form, spacy_status)] += 1\n",
        "\n",
        "# 2. Crea dataframe\n",
        "aggregated = []\n",
        "for (type_form, spacy_status), freq in sorted(counter.items()):\n",
        "    aggregated.append({\n",
        "        \"type\": type_form,\n",
        "        \"spacy_known\": spacy_status,\n",
        "        \"freq\": freq\n",
        "    })\n",
        "\n",
        "df_out = pd.DataFrame(aggregated)\n",
        "df_out = df_out.sort_values(by=[\"freq\", \"type\"], ascending=[False, True]).reset_index(drop=True)\n",
        "\n",
        "# 3. Salva CSV\n",
        "AGG_OUT_PATH = ROOT / \"results\" / \"parasintetici_finiti_si_types_spacy.csv\"\n",
        "df_out.to_csv(AGG_OUT_PATH, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Salvati {len(df_out)} types distinti con spacy_known in {AGG_OUT_PATH}\")"
      ],
      "metadata": {
        "id": "rGaPR2sNeBTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unifichiamo i file generati con le voci raggruppate per types."
      ],
      "metadata": {
        "id": "9Y8jGy-7cPYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = Path.cwd().resolve()\n",
        "FINITI_PATH = ROOT / \"results\" / \"parasintetici_finiti_types_spacy.csv\"\n",
        "INDEFINITI_PATH = ROOT / \"results\" / \"parasintetici_indefiniti_types_spacy.csv\"\n",
        "FINITI_SI_PATH = ROOT / \"results\" / \"parasintetici_finiti_si_types_spacy.csv\"\n",
        "OUT_PATH = ROOT / \"results\" / \"parasintetici_types_spacy_unificato.csv\"\n",
        "\n",
        "# Carica\n",
        "df_finiti = pd.read_csv(FINITI_PATH)\n",
        "df_indefiniti = pd.read_csv(INDEFINITI_PATH)\n",
        "df_finiti_si = pd.read_csv(FINITI_SI_PATH)\n",
        "\n",
        "# Unisci\n",
        "df_all = pd.concat([df_finiti, df_indefiniti, df_finiti_si], ignore_index=True)\n",
        "\n",
        "# Dizionario di aggregazione minimo per le tue colonne\n",
        "agg_dict = {\n",
        "    \"spacy_known\": \"max\",   # True se almeno una volta\n",
        "    \"freq\": \"sum\"           # somma delle frequenze su tutte le categorie\n",
        "}\n",
        "\n",
        "# Unifica per 'type'\n",
        "df_all_unique = (\n",
        "    df_all.groupby(\"type\", as_index=False)\n",
        "          .agg(agg_dict)\n",
        "          .sort_values(\"type\")\n",
        "          .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# Salva\n",
        "df_all_unique.to_csv(OUT_PATH, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"File unificato scritto in: {OUT_PATH}\")\n",
        "print(f\"Totale types unici: {len(df_all_unique)}\")\n"
      ],
      "metadata": {
        "id": "A5idnSI5cV2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calcoliamo ora le misure di precisione e recall per la capacità di spaCy di catturare veri negativi. La correzione manuale è stata fatta basando il confronto sulla presenza della parola nel GDLI (Battaglia). I risultati sono i seguenti (i valori fanno riferimento alle occorrenze dei tokens):\n",
        "\n",
        "1.   Forme non riconosciute (TRUE) = 88\n",
        "2.   Forme non riconosciute (FALSE) = 187\n",
        "3.   Forme riconosciute (TRUE) = 524\n",
        "4.   Forme riconosciute (FALSE) = 2\n",
        "\n",
        "Poiché l'interesse è nel valutare la capacità di individuare correttamente i true negatives, vengono invertite le classi di positività/negatività, in modo che 1 = TP, 2 = FP, 3 = TN, 4 = FN.  \n",
        "\n"
      ],
      "metadata": {
        "id": "r6b3yG2twGl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dati forniti\n",
        "FN = 187\n",
        "FP = 2\n",
        "TN = 88\n",
        "TP = 524\n",
        "TOTAL = FN + FP + TN + TP\n",
        "\n",
        "# === Inversione delle classi ===\n",
        "# Ora consideriamo come \"positivi\" i TN (prima negativi)\n",
        "# e come \"negativi\" i TP (prima positivi).\n",
        "\n",
        "# Rinominiamo i valori dopo l'inversione\n",
        "TP_inv = TN   # veri positivi diventano i vecchi TN\n",
        "TN_inv = TP   # veri negativi diventano i vecchi TP\n",
        "FP_inv = FN   # falsi positivi diventano i vecchi FN\n",
        "FN_inv = FP   # falsi negativi diventano i vecchi FP\n",
        "\n",
        "# Calcoli standard\n",
        "precision = TP_inv / (TP_inv + FP_inv)\n",
        "recall = TP_inv / (TP_inv + FN_inv)\n",
        "f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "print(\"=== Risultati con classi invertite ===\")\n",
        "print(f\"Precision: {precision:.3f}\")\n",
        "print(f\"Recall:    {recall:.3f}\")\n",
        "print(f\"F1-score:  {f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "eHIkE1Dw_66n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I risultati dimostrano l'affidabilità di spaCy nel catturare tutte le occorrenze di veri negativi (97,8%), ma scarsa precisione (32%), dati i numerosi casi di falsi negativi (quasi esclusivamente nelle forme finite). Poiché interessa una scrematura iniziale preliminare al controllo manuale, il metodo risulta affidabile, anche se testare altri metodi potrebbe portare a risultati ancora più efficienti.  "
      ],
      "metadata": {
        "id": "ynl1BIHYAJk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ora creiamo una lista con i risultati identificati come veri negativi, lemmatizzati alla forma dell'infinito. (Il file con i true negatives è caricato manualmente nella cartella 'results')."
      ],
      "metadata": {
        "id": "FYesuzk31iSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A questo punto verifichiamo quali types identificati non sono ricompresi nell'\"Onomaturgia darrighiana\" di G. Alvino (2012), e quali invece si trovano in Alvino e non sono stati catturati dal nostro codice. Poiché le nostre forme non sono state lemmatizzate, il confronto viene fatto sulla corrispondenza dei primi 4 caratteri"
      ],
      "metadata": {
        "id": "iTz8KSD4BCds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = Path.cwd().resolve()\n",
        "VERI_NEGATIVI_TXT = ROOT / \"results\" / \"veri_negativi.txt\"\n",
        "ALVINO_TXT       = ROOT / \"data\" / \"onomaturgia_alvino_parasint.txt\"\n",
        "\n",
        "OUT1 = ROOT / \"results\" / \"parasintetici_solo_in_veri_negativi.csv\"\n",
        "OUT2 = ROOT / \"results\" / \"parasintetici_solo_in_alvino.csv\"\n",
        "\n",
        "def load_types_txt(path: Path) -> set[str]:\n",
        "    \"\"\"Carica un file .txt con una voce per riga\"\"\"\n",
        "    types = set()\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            t = line.strip()\n",
        "            if not t:\n",
        "                continue\n",
        "            types.add(t.lower())\n",
        "    return types\n",
        "\n",
        "# --- Carica i dati (entrambi i file sono .txt)\n",
        "csv_types    = load_types_txt(VERI_NEGATIVI_TXT)\n",
        "alvino_types = load_types_txt(ALVINO_TXT)\n",
        "\n",
        "# --- Differenze\n",
        "solo_csv    = csv_types - alvino_types\n",
        "solo_alvino = alvino_types - csv_types\n",
        "\n",
        "# --- Salva risultati\n",
        "OUT1.parent.mkdir(parents=True, exist_ok=True)\n",
        "pd.DataFrame({\"type\": sorted(solo_csv)}).to_csv(OUT1, index=False, encoding=\"utf-8\")\n",
        "pd.DataFrame({\"type\": sorted(solo_alvino)}).to_csv(OUT2, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Salvati {len(solo_csv)} types solo in veri_negativi -> {OUT1}\")\n",
        "print(f\"Salvati {len(solo_alvino)} types solo in onomaturgia -> {OUT2}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "gpfogZu2A8bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reduplicati con funzione elativa"
      ],
      "metadata": {
        "id": "UuX8woOfiZbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proseguiamo ora con la ricerca dei reduplicati con funzione elativa. Applichiamo la REGEX al testo per trovare le forme reduplicate univerbate, e quindi le forme reduplicate con spazio."
      ],
      "metadata": {
        "id": "eNPZEOaSiQx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imposta la cartella di output\n",
        "OUTPUT_DIR = Path.cwd() / \"results\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Regex per riduplicazioni:\n",
        "# 1) identiche con o senza spazio (già esistente)\n",
        "pattern_exact = re.compile(r\"\\b(\\w+)[\\s]?\\1\\b\", flags=re.IGNORECASE)\n",
        "\n",
        "# 2) univerbate con vocale fusa: c r c r c (c = vocale, r = parte centrale)\n",
        "pattern_fused = re.compile(r\"\\b([aeiou])(\\w+)\\1\\2\\1\\b\", flags=re.IGNORECASE)\n",
        "\n",
        "# Tokenizzazione semplice per estrarre contesto\n",
        "words = re.findall(r\"\\w+|[^\\w\\s]\", text, flags=re.UNICODE)\n",
        "matches = []\n",
        "\n",
        "def clean_forma(s: str) -> str:\n",
        "    return s.rstrip(\".,;:!?…\").lower()\n",
        "\n",
        "for i in range(1, len(words) - 1):\n",
        "    word = words[i].lower()\n",
        "    prev_word = words[i-1].lower()\n",
        "    next_word = words[i+1].lower()\n",
        "\n",
        "    # --- UNIVERBATE ---\n",
        "    if re.fullmatch(pattern_exact, word) or re.fullmatch(pattern_fused, word):\n",
        "        forma = clean_forma(word)\n",
        "        if len(forma) >= 6:  # minimo 3+3\n",
        "            matches.append({\n",
        "                \"forma\": forma,\n",
        "                \"contesto\": \" \".join(words[max(0, i-5):i+6]),\n",
        "                \"tipo\": \"univerbata\"\n",
        "            })\n",
        "\n",
        "    # --- CON SPAZIO ---\n",
        "    bigram = f\"{prev_word} {word}\"\n",
        "    if re.fullmatch(pattern_exact, bigram):\n",
        "        forma = clean_forma(bigram)\n",
        "        if len(forma.replace(\" \", \"\")) >= 6:  # 3+3 senza contare lo spazio\n",
        "            matches.append({\n",
        "                \"forma\": forma,\n",
        "                \"contesto\": \" \".join(words[max(0, i-6):i+5]),\n",
        "                \"tipo\": \"con spazio\"\n",
        "            })\n",
        "\n",
        "# Percorso file output\n",
        "OUT_PATH = OUTPUT_DIR / \"forme_reduplicate_con_contesto.csv\"\n",
        "\n",
        "# Scrivi il CSV\n",
        "with open(OUT_PATH, mode=\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"forma\", \"tipo\", \"contesto\"])\n",
        "    writer.writeheader()\n",
        "    for row in matches:\n",
        "        writer.writerow(row)\n",
        "\n",
        "print(f\"File salvato in: {OUT_PATH}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "IZI7O3CBiQIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ora estraiamo solo le forme univerbate e raggruppiamole in types."
      ],
      "metadata": {
        "id": "Uc3e95dFN-pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Percorso file input creato dallo script precedente\n",
        "INPUT_PATH = Path.cwd() / \"results\" / \"forme_reduplicate_con_contesto.csv\"\n",
        "OUTPUT_PATH = Path.cwd() / \"results\" / \"forme_univerbate_frequenze.csv\"\n",
        "\n",
        "# Carica il CSV\n",
        "df = pd.read_csv(INPUT_PATH)\n",
        "\n",
        "# Filtra solo le forme univerbate\n",
        "df_univerbate = df[df[\"tipo\"] == \"univerbata\"].copy()\n",
        "\n",
        "# Normalizza la forma (minuscole, rimuovi punteggiatura finale se presente)\n",
        "df_univerbate[\"forma\"] = df_univerbate[\"forma\"].str.lower().str.replace(r\"[.,;:!?…]+$\", \"\", regex=True)\n",
        "\n",
        "# Conta i types e le frequenze\n",
        "frequenze = df_univerbate[\"forma\"].value_counts().reset_index()\n",
        "frequenze.columns = [\"forma\", \"frequenza\"]\n",
        "\n",
        "# Salva il CSV\n",
        "frequenze.to_csv(OUTPUT_PATH, index=False)\n",
        "print(f\"File salvato in: {OUTPUT_PATH}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_d-wppPDN_As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ora confrontiamo le voci trovate con quelle dell'*Onomaturgia darrighiana* di Alvino (2012)."
      ],
      "metadata": {
        "id": "w3u_7M04U8KL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = Path.cwd().resolve()\n",
        "ALVINO_TXT = ROOT / \"data\" / \"onomaturgia_alvino_reduplicati.txt\"\n",
        "OUT_SOLO_ALVINO = ROOT / \"results\" / \"reduplicati_solo_in_alvino.csv\"\n",
        "OUT_SOLO_UNIVERBATE = ROOT / \"results\" / \"reduplicati_solo_in_univerbate.csv\"\n",
        "\n",
        "# CARICA FREQUENZE (già creato sopra come OUTPUT_PATH)\n",
        "freq_df = pd.read_csv(OUTPUT_PATH)\n",
        "\n",
        "# CARICA LISTA ALVINO\n",
        "TRAIL_PUNCT_RE = re.compile(r\"[.,;:!?…]+$\")\n",
        "\n",
        "def load_types_txt(path: Path) -> set[str]:\n",
        "    types = set()\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            t = line.strip()\n",
        "            if not t:\n",
        "                continue\n",
        "            if t:\n",
        "                types.add(t)\n",
        "    return types\n",
        "\n",
        "alvino_set = load_types_txt(ALVINO_TXT)\n",
        "univerbate_set = set(freq_df[\"forma\"].tolist())\n",
        "\n",
        "# DIFFERENZE\n",
        "solo_in_alvino = sorted(alvino_set - univerbate_set)\n",
        "solo_in_univerbate = univerbate_set - alvino_set\n",
        "\n",
        "# SALVA FILES\n",
        "# 1) Solo in Alvino (senza frequenze)\n",
        "pd.DataFrame({\"forma\": solo_in_alvino}).to_csv(OUT_SOLO_ALVINO, index=False)\n",
        "\n",
        "# 2) Solo in forme_univerbate (con frequenze)\n",
        "df_solo_univerbate = freq_df[freq_df[\"forma\"].isin(solo_in_univerbate)].copy()\n",
        "df_solo_univerbate.sort_values([\"frequenza\", \"forma\"], ascending=[False, True], inplace=True)\n",
        "df_solo_univerbate.to_csv(OUT_SOLO_UNIVERBATE, index=False)\n",
        "\n",
        "print(f\"File salvati:\\n- Solo in Alvino: {OUT_SOLO_ALVINO}\\n- Solo in forme_univerbate: {OUT_SOLO_UNIVERBATE}\")"
      ],
      "metadata": {
        "id": "kR3VwbuAVFSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nella cartella 'results' della repository sulla piattaforma GitHub sono caricati i file di confronto annotati con le spiegazioni sul mancato match."
      ],
      "metadata": {
        "id": "RgVZGQBfZ5uk"
      }
    }
  ]
}