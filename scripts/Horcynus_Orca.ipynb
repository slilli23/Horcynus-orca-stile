{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVaQsYEro1TOMn0dqzUnyH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slilli23/Horcynus-orca-stile/blob/main/Horcynus_Orca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Esplorare l'anomalia: uno studio computazionale dello stile di Horcinus Orca\n",
        "di (anonimizzato)"
      ],
      "metadata": {
        "id": "RqGvf6P5DJAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il contributo propone un approccio innovativo allo studio linguistico di Horcynus Orca, integrando lettura ravvicinata e strumenti della linguistica computazionale. L’esplorazione del testo tramite la codifica di espressioni regolari (REGEX) consente di superare l’ostacolo costituito dall’estensione e complessità del romanzo – che ha spesso imposto un approccio esemplificativo – e incoraggia una formalizzazione in linea con la «grammaticalizzazione» dell’elemento deformante riconosciuta all’autore. L’analisi si concentra, a titolo esemplificativo, su due strategie inventive: le formazioni verbali parasintetiche riflessive e i reduplicati con funzione elativa. Il confronto con repertori esistenti ha rivelato nuove occorrenze riconducibili alla creatività autoriale, confermando che l’approccio integrato tra *distant* e *close reading* garantisce un’analisi più precisa e completa, aprendo nuove possibilità interpretative."
      ],
      "metadata": {
        "id": "7gks4xvCjoLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definizione delle REGEX per identificare i fenomeni di interesse nel testo:\n",
        "\n",
        "*   Formazioni verbali parasintetiche riflessive con prefisso in A(D)-\n",
        "\n",
        "    **\\ba([bcdfglmnpqrstvz])\\1[a-zà-ù]{4,}[ro]si\\b**\n",
        "\n",
        "    **\\b(s[’']a([bcdfglmnpqrstvz])\\2[a-zà-ù]{4,})**\n",
        "\n",
        "\n",
        "*   Reduplicati con funzione elativa\n",
        "\n",
        "    **\\b(\\w+)[\\s]?\\1\\b**\n",
        "\n",
        "Si procede quindi all'importazione del testo in formato .txt e all'estrazione dei risultati attraverso l'applicazione delle REGEX.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "get9_eYgES0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "imKyUepwjecw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTjbJf6pDFWM"
      },
      "outputs": [],
      "source": [
        "# Clona il la cartella dal repository GitHub\n",
        "REPO_URL  = #anonimizzato per peer review\"\n",
        "REPO_NAME = \"Horcynus-orca-stile\"\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Torna alla radice di Colab per sicurezza\n",
        "%cd /content\n",
        "\n",
        "if (Path(REPO_NAME) / \".git\").exists():\n",
        "    %cd {REPO_NAME}\n",
        "    !git pull --ff-only\n",
        "else:\n",
        "    !git clone --depth=1 {REPO_URL}\n",
        "    %cd {REPO_NAME}\n",
        "\n",
        "# Importa le librerie necessarie per l'analisi\n",
        "from pathlib import Path\n",
        "import re\n",
        "import pandas as pd\n",
        "import csv\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# Definisci le cartelle\n",
        "ROOT = Path.cwd()\n",
        "DATA_DIR = ROOT / \"data\"\n",
        "SCRIPTS_DIR = ROOT / \"scripts\"\n",
        "RESULTS_DIR = ROOT / \"results\"\n",
        "\n",
        "#Verifica l'esistenza delle cartelle e la correttezza dei percorsi\n",
        "RESULTS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"Root:\", ROOT)\n",
        "print(\"Data:\", DATA_DIR)\n",
        "print(\"Scripts:\", SCRIPTS_DIR)\n",
        "print(\"Results:\", RESULTS_DIR)\n",
        "\n",
        "# Carica il testo\n",
        "file_path = DATA_DIR / \"Horcynus_orca.txt\"\n",
        "\n",
        "with open(file_path, encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Lunghezza testo:\", len(text), \"caratteri\")\n",
        "print(\"Primi 500 caratteri:\\n\", text[:500])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parasintetici con prefisso in A(D)-"
      ],
      "metadata": {
        "id": "wHzPAz-fjVuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applichiamo la prima REGEX (verbi parasintetici riflessivi con prefisso in ad-, forma indefinita)."
      ],
      "metadata": {
        "id": "-1hTqvjRKv0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imposta la cartella di output\n",
        "OUTPUT_DIR = Path.cwd() / \"results\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Salva la REGEX in una variabile (raw string per non dover raddoppiare i backslash)\n",
        "pattern = r\"\\ba([bcdfglmnpqrstvz])\\1[a-zà-ù]{4,}[ro]si\\b\"\n",
        "rx = re.compile(pattern, flags=re.IGNORECASE | re.UNICODE)\n",
        "\n",
        "# Applica la REGEX e salva il match con il con contesto (5 parole prima e dopo)\n",
        "words = text.split()\n",
        "matches = []\n",
        "\n",
        "TRAIL_PUNCT_RE = re.compile(r\"[.,;:!?…]+$\")  # punteggiatura finale\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "    # 1) rimuovi punteggiatura finale\n",
        "    token = TRAIL_PUNCT_RE.sub(\"\", word)\n",
        "\n",
        "    # 2) separa per apostrofo\n",
        "    parts = re.split(r\"[’']\", token)\n",
        "\n",
        "    found_for_token = False\n",
        "    for part in parts:\n",
        "        if not part:\n",
        "            continue\n",
        "        if rx.search(part):\n",
        "            left_context = \" \".join(words[max(0, i-5):i])\n",
        "            right_context = \" \".join(words[i+1:i+6])\n",
        "            # salva SOLO la parte dopo l'apostrofo che ha matchato\n",
        "            matches.append([part, left_context, right_context])\n",
        "            found_for_token = True\n",
        "            break  # evita duplicati se la stessa parola produce più sottoparti\n",
        "\n",
        "# Definisci il file di output\n",
        "OUTPUT_PATH = OUTPUT_DIR / \"parasintetici_indefiniti.csv\"\n",
        "\n",
        "# Salva i risultati in CSV\n",
        "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(OUTPUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"match\", \"left_context\", \"right_context\"])\n",
        "    writer.writerows(matches)\n",
        "\n",
        "print(f\"Salvati {len(matches)} risultati in {OUTPUT_PATH}\")\n"
      ],
      "metadata": {
        "id": "zEKq5_Rw5U6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifichiamo se le parole trovate appartengono al vocabolario italiano, utilizzando la libreria spaCy. Per prima cosa scarichiamo il modello utilizzato da spaCy. Viene scelto il modello 'large', per ottenere una prestazione più accurata."
      ],
      "metadata": {
        "id": "DO8DU0DJgBVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installazione\n",
        "!pip -q install -U pip setuptools wheel\n",
        "!pip -q install \"numpy==1.26.4\" \"spacy==3.7.5\"\n",
        "!python -m spacy download it_core_news_lg\n",
        "\n",
        "\n",
        "# --- Setup ---\n",
        "import spacy"
      ],
      "metadata": {
        "id": "cmYacxvSq9vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Riavviamo il runtime per risolvere i problemi di dipendenze (Ctrl+M)."
      ],
      "metadata": {
        "id": "Jy1J73kiZcYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup ---\n",
        "import spacy"
      ],
      "metadata": {
        "id": "Yna2KoTYTBL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Annotiamo il testo con la libreria spaCy."
      ],
      "metadata": {
        "id": "wjZvcLKXZo1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = Path.cwd().resolve()\n",
        "IN_PATH  = ROOT / \"results\" / \"parasintetici_indefiniti.csv\"\n",
        "OUT_PATH = ROOT / \"results\" / \"parasintetici_indefiniti_spacy.csv\"\n",
        "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "nlp = spacy.load(\"it_core_news_lg\")\n",
        "\n",
        "TRAIL_PUNCT_RE = re.compile(r\"[.,;:!?…]+$\")\n",
        "\n",
        "def variants(w: str):\n",
        "    w0 = w.strip()\n",
        "    w0 = TRAIL_PUNCT_RE.sub(\"\", w0)\n",
        "    outs = set([w0, w0.lower(), w0.replace(\"’\",\"'\")])\n",
        "    # se riflessivo in -rsi prova senza 'si' (infinito)\n",
        "    wl = w0.lower().replace(\"’\",\"'\")\n",
        "    if wl.endswith(\"rsi\"):\n",
        "        outs.add(wl[:-2])\n",
        "    # rimuovi apostrofo iniziale di clitici frequenti\n",
        "    for pref in (\"l'\", \"s'\", \"m'\", \"t'\", \"d'\", \"n'\"):\n",
        "        if wl.startswith(pref):\n",
        "            outs.add(wl[len(pref):])\n",
        "    return outs\n",
        "\n",
        "def spacy_check(word: str):\n",
        "    # ordina preferendo la forma originale e la minuscola\n",
        "    cand = sorted(list(variants(word)), key=lambda x: (x != word, x != word.lower()))\n",
        "    known = False\n",
        "    chosen = None\n",
        "    for v in cand:\n",
        "        if not nlp.vocab[v].is_oov:\n",
        "            known = True\n",
        "            chosen = v\n",
        "            break\n",
        "    form = chosen or cand[0]\n",
        "    doc = nlp(form)\n",
        "     # usa il primo token \"parola\" (salta eventuali segni)\n",
        "    tok = next((t for t in doc if t.is_alpha or t.text.strip(\"'\")), doc[0])\n",
        "    return known, form, tok.lemma_\n",
        "\n",
        "rows_out = []\n",
        "with open(IN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    r = csv.DictReader(f)\n",
        "    for row in r:\n",
        "        w = row[\"match\"]\n",
        "        known, chosen, lemma = spacy_check(w)\n",
        "        row[\"spacy_known\"]   = known\n",
        "        row[\"chosen_variant\"] = chosen    # forma su cui è stato fatto il check\n",
        "        row[\"lemma\"]          = lemma\n",
        "        rows_out.append(row)\n",
        "\n",
        "# Scrivi output\n",
        "with open(OUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    fieldnames = list(rows_out[0].keys())\n",
        "    w = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "    w.writeheader()\n",
        "    w.writerows(rows_out)\n",
        "\n",
        "print(f\"Annotate {len(rows_out)} forme. File scritto in: {OUT_PATH}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "KU_Pfpaqc7sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accorpiamo le voci in types distinguendo le voci omografe che hanno avuto diverse etichette generate da spaCy."
      ],
      "metadata": {
        "id": "2BCojsCfbAjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conta per (type, spacy_known)\n",
        "counter = defaultdict(int)\n",
        "\n",
        "for row in rows_out:\n",
        "    type_form = row[\"match\"].strip().lower()\n",
        "    spacy_status = row[\"spacy_known\"]\n",
        "    counter[(type_form, spacy_status)] += 1\n",
        "\n",
        "# Crea DataFrame\n",
        "aggregated = []\n",
        "for (type_form, spacy_status), freq in sorted(counter.items()):\n",
        "    aggregated.append({\n",
        "        \"type\": type_form,\n",
        "        \"spacy_known\": spacy_status,\n",
        "        \"freq\": freq\n",
        "    })\n",
        "\n",
        "df_out = pd.DataFrame(aggregated)\n",
        "df_out = df_out.sort_values(by=[\"freq\", \"type\"], ascending=[False, True]).reset_index(drop=True)\n",
        "\n",
        "# --- Salva\n",
        "AGG_OUT_PATH = ROOT / \"results\" / \"parasintetici_indefiniti_types_spacy.csv\"\n",
        "df_out.to_csv(AGG_OUT_PATH, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Salvati {len(df_out)} types distinti con spacy_known in {AGG_OUT_PATH}\")\n"
      ],
      "metadata": {
        "id": "mfCM9NqwbBG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applichiamo la seconda REGEX (forme parasintetiche riflessive con prefisso A(D)- nelle forme finite del verbo."
      ],
      "metadata": {
        "id": "VWKXvr5Ck9fT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imposta la cartella di output\n",
        "OUTPUT_DIR = Path.cwd() / \"results\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Salva la REGEX in una variabile (raw string per non dover raddoppiare i backslash)\n",
        "pattern = r\"\\b(s[’']a([bcdfglmnpqrstvz])\\2[a-zà-ù]{4,})\"\n",
        "rx = re.compile(pattern, flags=re.IGNORECASE | re.UNICODE)\n",
        "\n",
        "# Applica la REGEX e salva il match con il con contesto (5 parole prima e dopo)\n",
        "words = text.split()\n",
        "matches = []\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "    if rx.search(word):\n",
        "        left_context = \" \".join(words[max(0, i-5):i])\n",
        "        right_context = \" \".join(words[i+1:i+6])\n",
        "        matches.append([word, left_context, right_context])\n",
        "\n",
        "# Definisci il file di output\n",
        "OUTPUT_PATH = OUTPUT_DIR / \"parasintetici_finiti.csv\"\n",
        "\n",
        "# Salva i risultati in CSV\n",
        "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(OUTPUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"match\", \"left_context\", \"right_context\"])\n",
        "    writer.writerows(matches)\n",
        "\n",
        "print(f\"Salvati {len(matches)} risultati in {OUTPUT_PATH}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "DDqR6BCZc9Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Annotiamo il testo attraverso spaCy per verificare le forme riconosciute come italiane."
      ],
      "metadata": {
        "id": "MfzsaYLGcAXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = Path.cwd().resolve()\n",
        "IN_PATH  = ROOT / \"results\" / \"parasintetici_finiti.csv\"\n",
        "OUT_PATH = ROOT / \"results\" / \"parasintetici_finiti_spacy.csv\"\n",
        "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "nlp = spacy.load(\"it_core_news_lg\")\n",
        "\n",
        "_APOS = r\"[’']\"  # apostrofo semplice o tipografico\n",
        "S_PREFIX_RE = re.compile(rf\"^\\s*s{_APOS}\\s*(.+)\\s*$\", flags=re.IGNORECASE)\n",
        "TRAIL_PUNCT_RE = re.compile(r\"[.,;:!?…]+$\")\n",
        "\n",
        "# considera solo la parola dopo la sequenza s'\n",
        "def extract_verb_form(w: str) -> str:\n",
        "    w0 = w.strip()\n",
        "    w0 = TRAIL_PUNCT_RE.sub(\"\", w0)\n",
        "    m = S_PREFIX_RE.match(w0)\n",
        "    if m:\n",
        "        return m.group(1).strip()\n",
        "    return w0\n",
        "\n",
        "def variants(w: str):\n",
        "    w0 = extract_verb_form(w).strip()\n",
        "    return {w0, w0.lower()}\n",
        "\n",
        "\n",
        "def spacy_check(word: str):\n",
        "\n",
        "    cand = sorted(list(variants(word)), key=lambda x: (x != word, x != word.lower()))\n",
        "    known = False\n",
        "    chosen = None\n",
        "    for v in cand:\n",
        "        if not nlp.vocab[v].is_oov:\n",
        "            known = True\n",
        "            chosen = v\n",
        "            break\n",
        "    form = chosen or cand[0]\n",
        "    doc = nlp(form)\n",
        "    tok = next((t for t in doc if t.is_alpha or t.text.strip(\"'\")), doc[0])\n",
        "    return known, form, tok.lemma_\n",
        "\n",
        "rows_out = []\n",
        "with open(IN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    r = csv.DictReader(f)\n",
        "    for row in r:\n",
        "        w = row[\"match\"]\n",
        "        known, chosen, lemma = spacy_check(w)\n",
        "        row[\"spacy_known\"]   = known\n",
        "        row[\"chosen_variant\"] = chosen    # forma su cui è stato fatto il check\n",
        "        row[\"lemma\"]          = lemma\n",
        "        rows_out.append(row)\n",
        "\n",
        "# Scrivi output\n",
        "with open(OUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    fieldnames = list(rows_out[0].keys())\n",
        "    w = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "    w.writeheader()\n",
        "    w.writerows(rows_out)\n",
        "\n",
        "print(f\"Annotate {len(rows_out)} forme. File scritto in: {OUT_PATH}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "QGe-rJJVk7V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accorpiamo le voci in types distinguendo le voci omografe che hanno avuto diverse etichette generate da spaCy."
      ],
      "metadata": {
        "id": "J4VdbuDcaZ0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Normalizza e aggrega\n",
        "counter = defaultdict(int)  # chiave = (type, spacy_known), valore = frequenza\n",
        "\n",
        "for row in rows_out:\n",
        "    type_form = extract_verb_form(row[\"match\"]).lower().strip()\n",
        "    spacy_status = row[\"spacy_known\"]\n",
        "    counter[(type_form, spacy_status)] += 1\n",
        "\n",
        "# 2. Crea dataframe\n",
        "aggregated = []\n",
        "for (type_form, spacy_status), freq in sorted(counter.items()):\n",
        "    aggregated.append({\n",
        "        \"type\": type_form,\n",
        "        \"spacy_known\": spacy_status,\n",
        "        \"freq\": freq\n",
        "    })\n",
        "\n",
        "df_out = pd.DataFrame(aggregated)\n",
        "df_out = df_out.sort_values(by=[\"freq\", \"type\"], ascending=[False, True]).reset_index(drop=True)\n",
        "\n",
        "# 3. Salva CSV\n",
        "AGG_OUT_PATH = ROOT / \"results\" / \"parasintetici_finiti_types_spacy.csv\"\n",
        "df_out.to_csv(AGG_OUT_PATH, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Salvati {len(df_out)} types distinti con spacy_known in {AGG_OUT_PATH}\")"
      ],
      "metadata": {
        "id": "TVwckCsRaZGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applichiamo la REGEX per trovare nel testo anche le sequenze con \"si\" non eliso."
      ],
      "metadata": {
        "id": "-RKjJhNbapJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, csv\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Input: il testo intero in una stringa ---\n",
        "# text = ...\n",
        "\n",
        "# Imposta la cartella di output\n",
        "OUTPUT_DIR = Path.cwd() / \"results\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# REGEX per la seconda parola: a + C + C + almeno 4 lettere\n",
        "AWORD_RE = re.compile(r\"^a([bcdfglmnpqrstvz])\\1[a-zà-ù]{4,}$\", flags=re.IGNORECASE | re.UNICODE)\n",
        "\n",
        "# Pulisci punteggiatura iniziale/finale e normalizza apostrofi\n",
        "TRAIL_PUNCT_RE = re.compile(r\"[.,;:!?…)\\]»”]+$\")\n",
        "LEAD_PUNCT_RE  = re.compile(r\"^[([«“\\\"(]+\")\n",
        "\n",
        "def clean_token(w: str) -> str:\n",
        "    if w is None:\n",
        "        return \"\"\n",
        "    w = str(w)\n",
        "    w = LEAD_PUNCT_RE.sub(\"\", w)\n",
        "    w = TRAIL_PUNCT_RE.sub(\"\", w)\n",
        "    return w.replace(\"’\", \"'\").strip()\n",
        "\n",
        "# Tokenizza in modo semplice (per contesti bastano gli spazi)\n",
        "words = text.split()\n",
        "matches = []\n",
        "\n",
        "i = 0\n",
        "while i < len(words) - 1:\n",
        "    w0_raw = words[i]\n",
        "    w1_raw = words[i+1]\n",
        "\n",
        "    w0 = clean_token(w0_raw)\n",
        "    w1 = clean_token(w1_raw)\n",
        "\n",
        "    # cerchiamo \"si\" come parola intera (case-insensitive)\n",
        "    if w0.lower() == \"si\" and AWORD_RE.match(w1):\n",
        "        left_context  = \" \".join(words[max(0, i-5):i])\n",
        "        # >>> salva SOLO la seconda parola (pulita) <<<\n",
        "        match_text    = w1\n",
        "        right_context = \" \".join(words[i+2:i+7])\n",
        "        matches.append([match_text, left_context, right_context])\n",
        "        i += 2\n",
        "        continue\n",
        "\n",
        "    i += 1\n",
        "\n",
        "# Definisci il file di output\n",
        "OUTPUT_PATH = OUTPUT_DIR / \"parasintetici_finiti_si.csv\"\n",
        "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Salva i risultati in CSV\n",
        "with open(OUTPUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"match\", \"left_context\", \"right_context\"])\n",
        "    writer.writerows(matches)\n",
        "\n",
        "print(f\"Salvati {len(matches)} risultati in {OUTPUT_PATH}\")"
      ],
      "metadata": {
        "id": "1jkzi0n2aveN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Annotiamo i risultati con la libreria spaCy."
      ],
      "metadata": {
        "id": "wT__WqScd1-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = Path.cwd().resolve()\n",
        "IN_PATH  = ROOT / \"results\" / \"parasintetici_finiti_si.csv\"\n",
        "OUT_PATH = ROOT / \"results\" / \"parasintetici_finiti_si_spacy.csv\"\n",
        "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "nlp = spacy.load(\"it_core_news_lg\")\n",
        "\n",
        "def variants(w: str):\n",
        "    w0 = w.strip()\n",
        "    return {w0, w0.lower()}\n",
        "\n",
        "def spacy_check(word: str):\n",
        "    cand = sorted(list(variants(word)), key=lambda x: (x != word, x != word.lower()))\n",
        "    known = False\n",
        "    chosen = None\n",
        "    for v in cand:\n",
        "        if not nlp.vocab[v].is_oov:\n",
        "            known = True\n",
        "            chosen = v\n",
        "            break\n",
        "    form = chosen or cand[0]\n",
        "    doc = nlp(form)\n",
        "    tok = next((t for t in doc if t.is_alpha or t.text.strip(\"'\")), doc[0])\n",
        "    return known, form, tok.lemma_\n",
        "\n",
        "rows_out = []\n",
        "with open(IN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    r = csv.DictReader(f)\n",
        "    for row in r:\n",
        "        w = row[\"match\"]\n",
        "        known, chosen, lemma = spacy_check(w)\n",
        "        row[\"spacy_known\"]    = known\n",
        "        row[\"chosen_variant\"] = chosen\n",
        "        row[\"lemma\"]          = lemma\n",
        "        rows_out.append(row)\n",
        "\n",
        "with open(OUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    fieldnames = list(rows_out[0].keys())\n",
        "    w = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "    w.writeheader()\n",
        "    w.writerows(rows_out)\n",
        "\n",
        "print(f\"Annotate {len(rows_out)} forme. File scritto in: {OUT_PATH}\")\n"
      ],
      "metadata": {
        "id": "s2UKtNPSd1ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accorpiamo le voci in types distinguendo le voci omografe che hanno avuto diverse etichette generate da spaCy."
      ],
      "metadata": {
        "id": "4RpECIPBzdkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Normalizza e aggrega\n",
        "counter = defaultdict(int)  # chiave = (type, spacy_known), valore = frequenza\n",
        "\n",
        "for row in rows_out:\n",
        "    type_form = extract_verb_form(row[\"match\"]).lower().strip()\n",
        "    spacy_status = row[\"spacy_known\"]\n",
        "    counter[(type_form, spacy_status)] += 1\n",
        "\n",
        "# 2. Crea dataframe\n",
        "aggregated = []\n",
        "for (type_form, spacy_status), freq in sorted(counter.items()):\n",
        "    aggregated.append({\n",
        "        \"type\": type_form,\n",
        "        \"spacy_known\": spacy_status,\n",
        "        \"freq\": freq\n",
        "    })\n",
        "\n",
        "df_out = pd.DataFrame(aggregated)\n",
        "df_out = df_out.sort_values(by=[\"freq\", \"type\"], ascending=[False, True]).reset_index(drop=True)\n",
        "\n",
        "# 3. Salva CSV\n",
        "AGG_OUT_PATH = ROOT / \"results\" / \"parasintetici_finiti_si_types_spacy.csv\"\n",
        "df_out.to_csv(AGG_OUT_PATH, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Salvati {len(df_out)} types distinti con spacy_known in {AGG_OUT_PATH}\")"
      ],
      "metadata": {
        "id": "rGaPR2sNeBTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unifichiamo i file generati con le voci raggruppate per types."
      ],
      "metadata": {
        "id": "9Y8jGy-7cPYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = Path.cwd().resolve()\n",
        "FINITI_PATH = ROOT / \"results\" / \"parasintetici_finiti_types_spacy.csv\"\n",
        "INDEFINITI_PATH = ROOT / \"results\" / \"parasintetici_indefiniti_types_spacy.csv\"\n",
        "FINITI_SI_PATH = ROOT / \"results\" / \"parasintetici_finiti_si_types_spacy.csv\"\n",
        "OUT_PATH = ROOT / \"results\" / \"parasintetici_types_spacy_unificato.csv\"\n",
        "\n",
        "# Carica\n",
        "df_finiti = pd.read_csv(FINITI_PATH)\n",
        "df_indefiniti = pd.read_csv(INDEFINITI_PATH)\n",
        "df_finiti_si = pd.read_csv(FINITI_SI_PATH)\n",
        "\n",
        "# Unisci\n",
        "df_all = pd.concat([df_finiti, df_indefiniti, df_finiti_si], ignore_index=True)\n",
        "\n",
        "# Dizionario di aggregazione minimo per le tue colonne\n",
        "agg_dict = {\n",
        "    \"spacy_known\": \"max\",   # True se almeno una volta\n",
        "    \"freq\": \"sum\"           # somma delle frequenze su tutte le categorie\n",
        "}\n",
        "\n",
        "# Unifica per 'type'\n",
        "df_all_unique = (\n",
        "    df_all.groupby(\"type\", as_index=False)\n",
        "          .agg(agg_dict)\n",
        "          .sort_values(\"type\")\n",
        "          .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# Salva\n",
        "df_all_unique.to_csv(OUT_PATH, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"File unificato scritto in: {OUT_PATH}\")\n",
        "print(f\"Totale types unici: {len(df_all_unique)}\")\n"
      ],
      "metadata": {
        "id": "A5idnSI5cV2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calcoliamo ora le misure di precisione e recall per la capacità di spaCy di catturare veri negativi. La correzione manuale è stata fatta basando il confronto sulla presenza della parola nel GDLI (Battaglia). I risultati sono i seguenti (i valori fanno riferimento alle occorrenze dei tokens):\n",
        "\n",
        "1.   Forme non riconosciute (TRUE) = 88\n",
        "2.   Forme non riconosciute (FALSE) = 187\n",
        "3.   Forme riconosciute (TRUE) = 524\n",
        "4.   Forme riconosciute (FALSE) = 2\n",
        "\n",
        "Poiché l'interesse è nel valutare la capacità di individuare correttamente i true negatives, vengono invertite le classi di positività/negatività, in modo che 1 = TP, 2 = FP, 3 = TN, 4 = FN.  \n",
        "\n"
      ],
      "metadata": {
        "id": "r6b3yG2twGl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dati forniti\n",
        "FN = 187\n",
        "FP = 2\n",
        "TN = 88\n",
        "TP = 524\n",
        "TOTAL = FN + FP + TN + TP\n",
        "\n",
        "# === Inversione delle classi ===\n",
        "# Ora consideriamo come \"positivi\" i TN (prima negativi)\n",
        "# e come \"negativi\" i TP (prima positivi).\n",
        "\n",
        "# Rinominiamo i valori dopo l'inversione\n",
        "TP_inv = TN   # veri positivi diventano i vecchi TN\n",
        "TN_inv = TP   # veri negativi diventano i vecchi TP\n",
        "FP_inv = FN   # falsi positivi diventano i vecchi FN\n",
        "FN_inv = FP   # falsi negativi diventano i vecchi FP\n",
        "\n",
        "# Calcoli standard\n",
        "precision = TP_inv / (TP_inv + FP_inv)\n",
        "recall = TP_inv / (TP_inv + FN_inv)\n",
        "f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "print(\"=== Risultati con classi invertite ===\")\n",
        "print(f\"Precision: {precision:.3f}\")\n",
        "print(f\"Recall:    {recall:.3f}\")\n",
        "print(f\"F1-score:  {f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "eHIkE1Dw_66n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I risultati dimostrano l'affidabilità di spaCy nel catturare tutte le occorrenze di veri negativi (97,8%), ma scarsa precisione (32%), dati i numerosi casi di falsi negativi (quasi esclusivamente nelle forme finite). Poiché interessa una scrematura iniziale preliminare al controllo manuale, il metodo risulta affidabile, anche se testare altri metodi potrebbe portare a risultati ancora più efficienti.  "
      ],
      "metadata": {
        "id": "ynl1BIHYAJk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ora creiamo una lista con i risultati identificati come veri negativi, lemmatizzati alla forma dell'infinito. (Il file con i true negatives è caricato manualmente nella cartella 'results')."
      ],
      "metadata": {
        "id": "FYesuzk31iSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A questo punto verifichiamo quali types identificati non sono ricompresi nell'\"Onomaturgia darrighiana\" di G. Alvino (2012), e quali invece si trovano in Alvino e non sono stati catturati dal nostro codice. Poiché le nostre forme non sono state lemmatizzate, il confronto viene fatto sulla corrispondenza dei primi 4 caratteri"
      ],
      "metadata": {
        "id": "iTz8KSD4BCds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = Path.cwd().resolve()\n",
        "VERI_NEGATIVI_TXT = ROOT / \"results\" / \"veri_negativi.txt\"\n",
        "ALVINO_TXT       = ROOT / \"data\" / \"onomaturgia_alvino_parasint.txt\"\n",
        "\n",
        "OUT1 = ROOT / \"results\" / \"parasintetici_solo_in_veri_negativi.csv\"\n",
        "OUT2 = ROOT / \"results\" / \"parasintetici_solo_in_alvino.csv\"\n",
        "\n",
        "def load_types_txt(path: Path) -> set[str]:\n",
        "    \"\"\"Carica un file .txt con una voce per riga\"\"\"\n",
        "    types = set()\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            t = line.strip()\n",
        "            if not t:\n",
        "                continue\n",
        "            types.add(t.lower())\n",
        "    return types\n",
        "\n",
        "# --- Carica i dati (entrambi i file sono .txt)\n",
        "csv_types    = load_types_txt(VERI_NEGATIVI_TXT)\n",
        "alvino_types = load_types_txt(ALVINO_TXT)\n",
        "\n",
        "# --- Differenze\n",
        "solo_csv    = csv_types - alvino_types\n",
        "solo_alvino = alvino_types - csv_types\n",
        "\n",
        "# --- Salva risultati\n",
        "OUT1.parent.mkdir(parents=True, exist_ok=True)\n",
        "pd.DataFrame({\"type\": sorted(solo_csv)}).to_csv(OUT1, index=False, encoding=\"utf-8\")\n",
        "pd.DataFrame({\"type\": sorted(solo_alvino)}).to_csv(OUT2, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Salvati {len(solo_csv)} types solo in veri_negativi -> {OUT1}\")\n",
        "print(f\"Salvati {len(solo_alvino)} types solo in onomaturgia -> {OUT2}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "gpfogZu2A8bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reduplicati con funzione elativa"
      ],
      "metadata": {
        "id": "UuX8woOfiZbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proseguiamo ora con la ricerca dei reduplicati con funzione elativa. Applichiamo la REGEX al testo per trovare le forme reduplicate univerbate, e quindi le forme reduplicate con spazio."
      ],
      "metadata": {
        "id": "eNPZEOaSiQx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imposta la cartella di output\n",
        "OUTPUT_DIR = Path.cwd() / \"results\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Regex per riduplicazioni:\n",
        "# 1) identiche con o senza spazio (già esistente)\n",
        "pattern_exact = re.compile(r\"\\b(\\w+)[\\s]?\\1\\b\", flags=re.IGNORECASE)\n",
        "\n",
        "# 2) univerbate con vocale fusa: c r c r c (c = vocale, r = parte centrale)\n",
        "pattern_fused = re.compile(r\"\\b([aeiou])(\\w+)\\1\\2\\1\\b\", flags=re.IGNORECASE)\n",
        "\n",
        "# Tokenizzazione semplice per estrarre contesto\n",
        "words = re.findall(r\"\\w+|[^\\w\\s]\", text, flags=re.UNICODE)\n",
        "matches = []\n",
        "\n",
        "def clean_forma(s: str) -> str:\n",
        "    return s.rstrip(\".,;:!?…\").lower()\n",
        "\n",
        "for i in range(1, len(words) - 1):\n",
        "    word = words[i].lower()\n",
        "    prev_word = words[i-1].lower()\n",
        "    next_word = words[i+1].lower()\n",
        "\n",
        "    # --- UNIVERBATE ---\n",
        "    if re.fullmatch(pattern_exact, word) or re.fullmatch(pattern_fused, word):\n",
        "        forma = clean_forma(word)\n",
        "        if len(forma) >= 6:  # minimo 3+3\n",
        "            matches.append({\n",
        "                \"forma\": forma,\n",
        "                \"contesto\": \" \".join(words[max(0, i-5):i+6]),\n",
        "                \"tipo\": \"univerbata\"\n",
        "            })\n",
        "\n",
        "    # --- CON SPAZIO ---\n",
        "    bigram = f\"{prev_word} {word}\"\n",
        "    if re.fullmatch(pattern_exact, bigram):\n",
        "        forma = clean_forma(bigram)\n",
        "        if len(forma.replace(\" \", \"\")) >= 6:  # 3+3 senza contare lo spazio\n",
        "            matches.append({\n",
        "                \"forma\": forma,\n",
        "                \"contesto\": \" \".join(words[max(0, i-6):i+5]),\n",
        "                \"tipo\": \"con spazio\"\n",
        "            })\n",
        "\n",
        "# Percorso file output\n",
        "OUT_PATH = OUTPUT_DIR / \"forme_reduplicate_con_contesto.csv\"\n",
        "\n",
        "# Scrivi il CSV\n",
        "with open(OUT_PATH, mode=\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"forma\", \"tipo\", \"contesto\"])\n",
        "    writer.writeheader()\n",
        "    for row in matches:\n",
        "        writer.writerow(row)\n",
        "\n",
        "print(f\"File salvato in: {OUT_PATH}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "IZI7O3CBiQIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ora estraiamo solo le forme univerbate e raggruppiamole in types."
      ],
      "metadata": {
        "id": "Uc3e95dFN-pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Percorso file input creato dallo script precedente\n",
        "INPUT_PATH = Path.cwd() / \"results\" / \"forme_reduplicate_con_contesto.csv\"\n",
        "OUTPUT_PATH = Path.cwd() / \"results\" / \"forme_univerbate_frequenze.csv\"\n",
        "\n",
        "# Carica il CSV\n",
        "df = pd.read_csv(INPUT_PATH)\n",
        "\n",
        "# Filtra solo le forme univerbate\n",
        "df_univerbate = df[df[\"tipo\"] == \"univerbata\"].copy()\n",
        "\n",
        "# Normalizza la forma (minuscole, rimuovi punteggiatura finale se presente)\n",
        "df_univerbate[\"forma\"] = df_univerbate[\"forma\"].str.lower().str.replace(r\"[.,;:!?…]+$\", \"\", regex=True)\n",
        "\n",
        "# Conta i types e le frequenze\n",
        "frequenze = df_univerbate[\"forma\"].value_counts().reset_index()\n",
        "frequenze.columns = [\"forma\", \"frequenza\"]\n",
        "\n",
        "# Salva il CSV\n",
        "frequenze.to_csv(OUTPUT_PATH, index=False)\n",
        "print(f\"File salvato in: {OUTPUT_PATH}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_d-wppPDN_As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ora confrontiamo le voci trovate con quelle dell'*Onomaturgia darrighiana* di Alvino (2012)."
      ],
      "metadata": {
        "id": "w3u_7M04U8KL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = Path.cwd().resolve()\n",
        "ALVINO_TXT = ROOT / \"data\" / \"onomaturgia_alvino_reduplicati.txt\"\n",
        "OUT_SOLO_ALVINO = ROOT / \"results\" / \"reduplicati_solo_in_alvino.csv\"\n",
        "OUT_SOLO_UNIVERBATE = ROOT / \"results\" / \"reduplicati_solo_in_univerbate.csv\"\n",
        "\n",
        "# CARICA FREQUENZE (già creato sopra come OUTPUT_PATH)\n",
        "freq_df = pd.read_csv(OUTPUT_PATH)\n",
        "\n",
        "# CARICA LISTA ALVINO\n",
        "TRAIL_PUNCT_RE = re.compile(r\"[.,;:!?…]+$\")\n",
        "\n",
        "def load_types_txt(path: Path) -> set[str]:\n",
        "    types = set()\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            t = line.strip()\n",
        "            if not t:\n",
        "                continue\n",
        "            if t:\n",
        "                types.add(t)\n",
        "    return types\n",
        "\n",
        "alvino_set = load_types_txt(ALVINO_TXT)\n",
        "univerbate_set = set(freq_df[\"forma\"].tolist())\n",
        "\n",
        "# DIFFERENZE\n",
        "solo_in_alvino = sorted(alvino_set - univerbate_set)\n",
        "solo_in_univerbate = univerbate_set - alvino_set\n",
        "\n",
        "# SALVA FILES\n",
        "# 1) Solo in Alvino (senza frequenze)\n",
        "pd.DataFrame({\"forma\": solo_in_alvino}).to_csv(OUT_SOLO_ALVINO, index=False)\n",
        "\n",
        "# 2) Solo in forme_univerbate (con frequenze)\n",
        "df_solo_univerbate = freq_df[freq_df[\"forma\"].isin(solo_in_univerbate)].copy()\n",
        "df_solo_univerbate.sort_values([\"frequenza\", \"forma\"], ascending=[False, True], inplace=True)\n",
        "df_solo_univerbate.to_csv(OUT_SOLO_UNIVERBATE, index=False)\n",
        "\n",
        "print(f\"File salvati:\\n- Solo in Alvino: {OUT_SOLO_ALVINO}\\n- Solo in forme_univerbate: {OUT_SOLO_UNIVERBATE}\")"
      ],
      "metadata": {
        "id": "kR3VwbuAVFSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nella cartella 'results' della repository sulla piattaforma GitHub sono caricati i file di confronto annotati con le spiegazioni sul mancato match."
      ],
      "metadata": {
        "id": "RgVZGQBfZ5uk"
      }
    }
  ]
}
